########################## Metricbeat Configuration ###########################

# This file is a full configuration example documenting all non-deprecated
# options in comments. For a shorter configuration example, that contains only
# the most common options, please see metricbeat.yml in the same directory.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/metricbeat/index.html

#============================  Config Reloading ===============================

# Config reloading allows to dynamically load modules. Each file which is
# monitored must contain one or multiple modules as a list.
metricbeat.config.modules:

  # Glob pattern for configuration reloading
  path: /etc/rancher-conf/metricbeat/*.yml

  # Period on which files under path should be checked for changes
  reload.period: 30s

  # Set to true to enable config reloading
  reload.enabled: true

# Maximum amount of time to randomly delay the start of a metricset. Use 0 to
# disable startup delay.
metricbeat.max_start_delay: 10s

#================================ General ======================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
# If this options is not defined, the hostname is used.

{{- with host }}
name: {{ .Name }}
{{- end }}
# The tags of the shipper are included in their own field with each
# transaction published. Tags make it easy to group servers by different
# logical properties.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output. Fields can be scalar values, arrays, dictionaries, or any nested
# combination of these.
fields:
{{- with host }}
{{- range $key, $value := .Labels }}
  {{ $key }}: "{{ $value }}"
{{- end }}
{{- end }}

http.enabled: true

# If this option is set to true, the custom fields are stored as top-level
# fields in the output document instead of being grouped under a fields
# sub-dictionary. Default is false.
#fields_under_root: false

# Internal queue configuration for buffering events to be published.
#queue:
  # Queue type by name (default 'mem')
  # The memory queue will present all available events (up to the outputs
  # bulk_max_size) to the output, the moment the output is ready to server
  # another batch of events.
  #mem:
    # Max number of events the queue can buffer.
    #events: 4096

    # Hints the minimum number of events stored in the queue,
    # before providing a batch of events to the outputs.
    # The default value is set to 2048.
    # A value of 0 ensures events are immediately available
    # to be sent to the outputs.
    #flush.min_events: 2048

    # Maximum duration after which events are available to the outputs,
    # if the number of events stored in the queue is < min_flush_events.
    #flush.timeout: 1s

  # The spool queue will store events in a local spool file, before
  # forwarding the events to the outputs.
  #
  # Beta: spooling to disk is currently a beta feature. Use with care.
  #
  # The spool file is a circular buffer, which blocks once the file/buffer is full.
  # Events are put into a write buffer and flushed once the write buffer
  # is full or the flush_timeout is triggered.
  # Once ACKed by the output, events are removed immediately from the queue,
  # making space for new events to be persisted.
  #spool:
    # The file namespace configures the file path and the file creation settings.
    # Once the file exists, the `size`, `page_size` and `prealloc` settings
    # will have no more effect.
    #file:
      # Location of spool file. The default value is ${path.data}/spool.dat.
      #path: "${path.data}/spool.dat"

      # Configure file permissions if file is created. The default value is 0600.
      #permissions: 0600

      # File size hint. The spool blocks, once this limit is reached. The default value is 100 MiB.
      #size: 100MiB

      # The files page size. A file is split into multiple pages of the same size. The default value is 4KiB.
      #page_size: 4KiB

      # If prealloc is set, the required space for the file is reserved using
      # truncate. The default value is true.
      #prealloc: true

    # Spool writer settings
    # Events are serialized into a write buffer. The write buffer is flushed if:
    # - The buffer limit has been reached.
    # - The configured limit of buffered events is reached.
    # - The flush timeout is triggered.
    #write:
      # Sets the write buffer size.
      #buffer_size: 1MiB

      # Maximum duration after which events are flushed if the write buffer
      # is not full yet. The default value is 1s.
      #flush.timeout: 1s

      # Number of maximum buffered events. The write buffer is flushed once the
      # limit is reached.
      #flush.events: 16384

      # Configure the on-disk event encoding. The encoding can be changed
      # between restarts.
      # Valid encodings are: json, ubjson, and cbor.
      #codec: cbor
    #read:
      # Reader flush timeout, waiting for more events to become available, so
      # to fill a complete batch as required by the outputs.
      # If flush_timeout is 0, all available events are forwarded to the
      # outputs immediately.
      # The default value is 0s.
      #flush.timeout: 0s

processors:
  # The following example enriches each event with metadata from the cloud
  # provider about the host machine. It works on EC2, GCE, DigitalOcean,
  # Tencent Cloud, and Alibaba Cloud.
  - add_cloud_metadata: ~

  # The following example enriches each event with the machine's local time zone
  # offset from UTC.
  - add_locale:
     format: offset

  # The following example enriches each event with docker metadata, it matches
  # given fields to an existing container id and adds info from that container:
  - add_docker_metadata:
     host: "unix:///var/run/docker.sock"
     match_fields: ["system.process.cgroup.id"]
     match_pids: ["process.pid", "process.ppid"]
     match_source: true
     match_source_index: 4
     match_short_id: false
     cleanup_timeout: 60
     labels.dedot: false

  # The following example enriches each event with host metadata.
  - add_host_metadata:
      netinfo.enabled: false

  - drop_fields:
      fields:
        - system.process.cgroup.blkio.id
        - system.process.cgroup.blkio.path
        - system.process.cgroup.blkio.total.bytes
        - system.process.cgroup.blkio.total.ios
        - system.process.cgroup.cpu.cfs.period.us
        - system.process.cgroup.cpu.cfs.quota.us
        - system.process.cgroup.cpu.cfs.shares
        - system.process.cgroup.cpu.id
        - system.process.cgroup.cpu.path
        - system.process.cgroup.cpu.rt.period.us
        - system.process.cgroup.cpu.rt.runtime.us
        - system.process.cgroup.cpu.stats.periods
        - system.process.cgroup.cpu.stats.throttled.ns
        - system.process.cgroup.cpu.stats.throttled.periods
        - system.process.cgroup.cpuacct.id
        - system.process.cgroup.cpuacct.path
        - system.process.cgroup.cpuacct.stats.system.ns
        - system.process.cgroup.cpuacct.stats.user.ns
        - system.process.cgroup.cpuacct.total.ns
        - system.process.cgroup.memory.id
        - system.process.cgroup.memory.kmem.failures
        - system.process.cgroup.memory.kmem.limit.bytes
        - system.process.cgroup.memory.kmem.usage.bytes
        - system.process.cgroup.memory.kmem.usage.max.bytes
        - system.process.cgroup.memory.kmem_tcp.failures
        - system.process.cgroup.memory.kmem_tcp.limit.bytes
        - system.process.cgroup.memory.kmem_tcp.usage.bytes
        - system.process.cgroup.memory.kmem_tcp.usage.max.bytes
        - system.process.cgroup.memory.mem.failures
        - system.process.cgroup.memory.mem.limit.bytes
        - system.process.cgroup.memory.mem.usage.bytes
        - system.process.cgroup.memory.mem.usage.max.bytes
        - system.process.cgroup.memory.memsw.failures
        - system.process.cgroup.memory.memsw.limit.bytes
        - system.process.cgroup.memory.memsw.usage.bytes
        - system.process.cgroup.memory.memsw.usage.max.bytes
        - system.process.cgroup.memory.path
        - system.process.cgroup.memory.stats.active_anon.bytes
        - system.process.cgroup.memory.stats.active_file.bytes
        - system.process.cgroup.memory.stats.cache.bytes
        - system.process.cgroup.memory.stats.hierarchical_memory_limit.bytes
        - system.process.cgroup.memory.stats.hierarchical_memsw_limit.bytes
        - system.process.cgroup.memory.stats.inactive_anon.bytes
        - system.process.cgroup.memory.stats.inactive_file.bytes
        - system.process.cgroup.memory.stats.major_page_faults
        - system.process.cgroup.memory.stats.mapped_file.bytes
        - system.process.cgroup.memory.stats.page_faults
        - system.process.cgroup.memory.stats.pages_in
        - system.process.cgroup.memory.stats.pages_out
        - system.process.cgroup.memory.stats.rss.bytes
        - system.process.cgroup.memory.stats.rss_huge.bytes
        - system.process.cgroup.memory.stats.swap.bytes
        - system.process.cgroup.memory.stats.unevictable.bytes
        - system.process.cgroup.path

  # The following example renames the field a to b:
  - rename:
      ignore_missing: true
      fail_on_error: false
      fields:
        - from: docker.container.labels.io.rancher.container.name
          to: container.name

# The directory from where to read the dashboards. The default is the `kibana`
# folder in the home path.
setup.dashboards.directory: /usr/share/metricbeat/kibana
