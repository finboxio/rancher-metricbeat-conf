########################## Metricbeat Configuration ###########################

# This file is a full configuration example documenting all non-deprecated
# options in comments. For a shorter configuration example, that contains only
# the most common options, please see metricbeat.yml in the same directory.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/metricbeat/index.html

#============================  Config Reloading ===============================

# Config reloading allows to dynamically load modules. Each file which is
# monitored must contain one or multiple modules as a list.
metricbeat.config.modules:

  # Glob pattern for configuration reloading
  path: /etc/rancher-conf/metricbeat/*.yml

  # Period on which files under path should be checked for changes
  reload.period: 30s

  # Set to true to enable config reloading
  reload.enabled: true

# Maximum amount of time to randomly delay the start of a metricset. Use 0 to
# disable startup delay.
metricbeat.max_start_delay: 10s

#================================ General ======================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
# If this options is not defined, the hostname is used.

{{- with host }}
name: {{ .Name }}
{{- end }}
# The tags of the shipper are included in their own field with each
# transaction published. Tags make it easy to group servers by different
# logical properties.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output. Fields can be scalar values, arrays, dictionaries, or any nested
# combination of these.
fields:
{{- with host }}
{{- range $key, $value := .Labels }}
  {{ $key }}: "{{ $value }}"
{{- end }}
{{- end }}

http.enabled: true

#==========================  Modules configuration ============================
metricbeat.modules:
  - module: beat
    metricsets: [ stats, state ]
    period: 1m
    hosts: [ localhost:5066 ]
    xpack.enabled: true

  #------------------------------- System Module -------------------------------
  - module: system
    enabled: true
    period: 30s
    metricsets:
      - cpu             # CPU usage
      - memory          # Memory usage
      - process         # Per process metrics
      - diskio          # Disk IO
      - socket          # Sockets and connection info (linux only)
      - socket_summary  # Socket summary
      - load            # CPU load averages
      - core            # Per CPU core usage

    processes: [ '.*' ]
    process.include_cpu_ticks: false
    process.cgroups.enabled: true
    process.include_top_n:
      enabled: true
      by_cpu: 10
      by_memory: 10

    cpu.metrics:  [ 'percentages', 'normalized_percentages' ]
    core.metrics: [ 'percentages' ]

    socket.reverse_lookup.enabled: true
    # socket.reverse_lookup.success_ttl: 60s
    # socket.reverse_lookup.failure_ttl: 60s

  - module: system
    enabled: true
    period: 60s
    metricsets:
      - process_summary # Process summary
      - network         # Network IO
      - uptime          # System Uptime
      - filesystem      # File system usage for each mountpoint
      - fsstat          # File system summary metrics

    # A list of filesystem types to ignore. The filesystem metricset will not
    # collect data from filesystems matching any of the specified types, and
    # fsstats will not include data from these filesystems in its summary stats.
    # If not set, types associated to virtual filesystems are automatically
    # added when this information is available in the system (e.g. the list of
    # `nodev` types in `/proc/filesystem`).
    filesystem.ignore_types:
      - nsfs
      - sysfs
      - rootfs
      - tmpfs
      - bdev
      - proc
      - cpuset
      - cgroup
      - cgroup2
      - devtmpfs
      - binfmt_misc
      - debugfs
      - tracefs
      - securityfs
      - sockfs
      - dax
      - bpf
      - pipefs
      - ramfs
      - hugetlbfs
      - devpts
      - ecryptfs
      - fuse
      - fusectl
      - overlay
      - pstore
      - mqueue
      - selinuxfs

    # If false, cmdline of a process is not cached.
    # process.cmdline.cache.enabled: true

    # A list of regular expressions used to whitelist environment variables
    # reported with the process metricset's events. Defaults to empty.
    # process.env.whitelist: []

    # Include the cumulative CPU tick values with the process metrics. Defaults
    # to false.

    # Raid mount point to monitor
    # raid.mount_point: '/'

    # Diskio configurations
    # diskio.include_devices: []

  #------------------------------- Docker Module -------------------------------
  - module: docker
    enabled: true
    period: 30s
    hosts: [ "unix:///var/run/docker.sock" ]
    cpu.cores: true
    labels.dedot: false
    metricsets:
      - cpu
      - diskio
      - memory
      - event
      - healthcheck

  - module: docker
    enabled: true
    period: 60s
    hosts: [ "unix:///var/run/docker.sock" ]
    cpu.cores: true
    labels.dedot: false
    metricsets:
      - network
      - info
      - container
      - image

{{- with host }}
{{- $host_id := .UUID }}
{{- with service }}
{{- with .Parent }}
{{- $my_stack := .Stack }}
{{- $my_service := .Name }}
{{- range $service := services }}
{{- range $key, $value := .Labels }}
  {{- $list := split $key "." }}
  {{- if gt (len $list) 2 }}
  {{- if (and (eq (index $list 0) $my_stack) (eq (index $list 1) $my_service) (eq (index $list 2) "module")) }}

  - module: {{ $value }}
    service.name: {{ $service.Name }}.{{ $service.Stack }}.rancher.internal
    period: 1m
    {{- if (lt (len $service.Containers) 1) }}
    enabled: false
    {{- else }}
    {{- $primary := index $service.Containers 0 }}
    {{- if or (ne $primary.State "running") (ne $primary.Host.UUID $host_id) }}
    enabled: false
    {{- else }}
    enabled: true
    {{- end }}
    hosts:
    {{- $hostLabel := $service.Labels.GetValue (printf "%s.%s.address" $my_stack $my_service) "${ip}" }}
    {{- range $service.Containers }}
      - {{ replace $hostLabel "${ip}" .Address }}
    {{- end }}
    {{- end }}
  {{- end }}
  {{- end }}
{{- end }}
{{- range $key, $value := .Labels }}
  {{- $list := split $key "." }}
  {{- if gt (len $list) 2 }}
  {{- if (and (eq (index $list 0) $my_stack) (eq (index $list 1) $my_service) (ne (index $list 2) "module") (ne (index $list 2) "address")) }}
    {{ replace $key (print $my_stack "." $my_service ".") "" 1 }}: {{ $value }}
  {{- end }}
  {{- end }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}


  # #------------------------------ Graphite Module ------------------------------
  # - module: graphite
  #   metricsets: ["server"]
  #   enabled: true

  #   # Host address to listen on. Default localhost.
  #   #host: localhost

  #   # Listening port. Default 2003.
  #   #port: 2003

  #   # Protocol to listen on. This can be udp or tcp. Default udp.
  #   #protocol: "udp"

  #   # Receive buffer size in bytes
  #   #receive_buffer_size: 1024

  #   #templates:
  #   #  - filter: "test.*.bash.*" # This would match metrics like test.localhost.bash.stats
  #   #    namespace: "test"
  #   #    template: ".host.shell.metric*" # test.localhost.bash.stats would become metric=stats and tags host=localhost,shell=bash
  #   #    delimiter: "_"

  # #------------------------------- HAProxy Module ------------------------------
  # - module: haproxy
  #   metricsets: ["info", "stat"]
  #   period: 10s
  #   hosts: ["tcp://127.0.0.1:14567"]
  #   enabled: true

  # #-------------------------------- HTTP Module --------------------------------
  # - module: http
  #   #metricsets:
  #   #  - json
  #   period: 10s
  #   hosts: ["localhost:80"]
  #   namespace: "json_namespace"
  #   path: "/"
  #   #body: ""
  #   #method: "GET"
  #   #username: "user"
  #   #password: "secret"
  #   #request.enabled: false
  #   #response.enabled: false
  #   #json.is_array: false
  #   #dedot.enabled: false

  # - module: http
  #   #metricsets:
  #   #  - server
  #   host: "localhost"
  #   port: "8080"
  #   enabled: false
  #   #paths:
  #   #  - path: "/foo"
  #   #    namespace: "foo"
  #   #    fields: # added to the the response in root. overwrites existing fields
  #   #      key: "value"

  # #------------------------------- MongoDB Module ------------------------------
  # - module: mongodb
  #   metricsets: ["dbstats", "status", "collstats", "metrics", "replstatus"]
  #   period: 10s
  #   enabled: true

  #   # The hosts must be passed as MongoDB URLs in the format:
  #   # [mongodb://][user:pass@]host[:port].
  #   # The username and password can also be set using the respective configuration
  #   # options. The credentials in the URL take precedence over the username and
  #   # password configuration options.
  #   hosts: ["localhost:27017"]

  #   # Optional SSL. By default is off.
  #   #ssl.enabled: true

  #   # Mode of verification of server certificate ('none' or 'full')
  #   #ssl.verification_mode: 'full'

  #   # List of root certificates for TLS server verifications
  #   #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  #   # Certificate for SSL client authentication
  #   #ssl.certificate: "/etc/pki/client/cert.pem"

  #   # Client Certificate Key
  #   #ssl.key: "/etc/pki/client/cert.key"

  #   # Username to use when connecting to MongoDB. Empty by default.
  #   #username: user

  #   # Password to use when connecting to MongoDB. Empty by default.
  #   #password: pass

  # #-------------------------------- MySQL Module -------------------------------
  # - module: mysql
  #   metricsets:
  #     - "status"
  #   #  - "galera_status"
  #   period: 10s

  #   # Host DSN should be defined as "user:pass@tcp(127.0.0.1:3306)/"
  #   # The username and password can either be set in the DSN or using the username
  #   # and password config options. Those specified in the DSN take precedence.
  #   hosts: ["root:secret@tcp(127.0.0.1:3306)/"]

  #   # Username of hosts. Empty by default.
  #   #username: root

  #   # Password of hosts. Empty by default.
  #   #password: secret

  #   # By setting raw to true, all raw fields from the status metricset will be added to the event.
  #   #raw: false

  # #----------------------------- PostgreSQL Module -----------------------------
  # - module: postgresql
  #   enabled: true
  #   metricsets:
  #     # Stats about every PostgreSQL database
  #     - database

  #     # Stats about the background writer process's activity
  #     - bgwriter

  #     # Stats about every PostgreSQL process
  #     - activity

  #   period: 10s

  #   # The host must be passed as PostgreSQL URL. Example:
  #   # postgres://localhost:5432?sslmode=disable
  #   # The available parameters are documented here:
  #   # https://godoc.org/github.com/lib/pq#hdr-Connection_String_Parameters
  #   hosts: ["postgres://localhost:5432"]

  #   # Username to use when connecting to PostgreSQL. Empty by default.
  #   #username: user

  #   # Password to use when connecting to PostgreSQL. Empty by default.
  #   #password: pass

  # #----------------------------- Prometheus Module -----------------------------
  # - module: prometheus
  #   metricsets: ["stats"]
  #   enabled: true
  #   period: 10s
  #   hosts: ["localhost:9090"]
  #   #metrics_path: /metrics
  #   #namespace: example

  # - module: prometheus
  #   metricsets: ["collector"]
  #   enabled: true
  #   period: 10s
  #   hosts: ["localhost:9090"]
  #   #metrics_path: /metrics
  #   #namespace: example

  #   # This can be used for service account based authorization:
  #   #  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  #   #ssl.certificate_authorities:
  #   #  - /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt

  # #-------------------------------- Redis Module -------------------------------
  # - module: redis
  #   metricsets: ["info", "keyspace"]
  #   enabled: true
  #   period: 10s

  #   # Redis hosts
  #   hosts: ["127.0.0.1:6379"]

  #   # Timeout after which time a metricset should return an error
  #   # Timeout is by default defined as period, as a fetch of a metricset
  #   # should never take longer then period, as otherwise calls can pile up.
  #   #timeout: 1s

  #   # Optional fields to be added to each event
  #   #fields:
  #   #  datacenter: west

  #   # Network type to be used for redis connection. Default: tcp
  #   #network: tcp

  #   # Max number of concurrent connections. Default: 10
  #   #maxconn: 10

  #   # Filters can be used to reduce the number of fields sent.
  #   #processors:
  #   #  - include_fields:
  #   #      fields: ["beat", "metricset", "redis.info.stats"]

  #   # Redis AUTH password. Empty by default.
  #   #password: foobared

# If this option is set to true, the custom fields are stored as top-level
# fields in the output document instead of being grouped under a fields
# sub-dictionary. Default is false.
#fields_under_root: false

# Internal queue configuration for buffering events to be published.
#queue:
  # Queue type by name (default 'mem')
  # The memory queue will present all available events (up to the outputs
  # bulk_max_size) to the output, the moment the output is ready to server
  # another batch of events.
  #mem:
    # Max number of events the queue can buffer.
    #events: 4096

    # Hints the minimum number of events stored in the queue,
    # before providing a batch of events to the outputs.
    # The default value is set to 2048.
    # A value of 0 ensures events are immediately available
    # to be sent to the outputs.
    #flush.min_events: 2048

    # Maximum duration after which events are available to the outputs,
    # if the number of events stored in the queue is < min_flush_events.
    #flush.timeout: 1s

  # The spool queue will store events in a local spool file, before
  # forwarding the events to the outputs.
  #
  # Beta: spooling to disk is currently a beta feature. Use with care.
  #
  # The spool file is a circular buffer, which blocks once the file/buffer is full.
  # Events are put into a write buffer and flushed once the write buffer
  # is full or the flush_timeout is triggered.
  # Once ACKed by the output, events are removed immediately from the queue,
  # making space for new events to be persisted.
  #spool:
    # The file namespace configures the file path and the file creation settings.
    # Once the file exists, the `size`, `page_size` and `prealloc` settings
    # will have no more effect.
    #file:
      # Location of spool file. The default value is ${path.data}/spool.dat.
      #path: "${path.data}/spool.dat"

      # Configure file permissions if file is created. The default value is 0600.
      #permissions: 0600

      # File size hint. The spool blocks, once this limit is reached. The default value is 100 MiB.
      #size: 100MiB

      # The files page size. A file is split into multiple pages of the same size. The default value is 4KiB.
      #page_size: 4KiB

      # If prealloc is set, the required space for the file is reserved using
      # truncate. The default value is true.
      #prealloc: true

    # Spool writer settings
    # Events are serialized into a write buffer. The write buffer is flushed if:
    # - The buffer limit has been reached.
    # - The configured limit of buffered events is reached.
    # - The flush timeout is triggered.
    #write:
      # Sets the write buffer size.
      #buffer_size: 1MiB

      # Maximum duration after which events are flushed if the write buffer
      # is not full yet. The default value is 1s.
      #flush.timeout: 1s

      # Number of maximum buffered events. The write buffer is flushed once the
      # limit is reached.
      #flush.events: 16384

      # Configure the on-disk event encoding. The encoding can be changed
      # between restarts.
      # Valid encodings are: json, ubjson, and cbor.
      #codec: cbor
    #read:
      # Reader flush timeout, waiting for more events to become available, so
      # to fill a complete batch as required by the outputs.
      # If flush_timeout is 0, all available events are forwarded to the
      # outputs immediately.
      # The default value is 0s.
      #flush.timeout: 0s

processors:
  # The following example enriches each event with metadata from the cloud
  # provider about the host machine. It works on EC2, GCE, DigitalOcean,
  # Tencent Cloud, and Alibaba Cloud.
  - add_cloud_metadata: ~

  # The following example enriches each event with the machine's local time zone
  # offset from UTC.
  - add_locale:
     format: offset

  # The following example enriches each event with docker metadata, it matches
  # given fields to an existing container id and adds info from that container:
  - add_docker_metadata:
     host: "unix:///var/run/docker.sock"
     match_fields: ["system.process.cgroup.id"]
     match_pids: ["process.pid", "process.ppid"]
     match_source: true
     match_source_index: 4
     match_short_id: false
     cleanup_timeout: 60
     labels.dedot: false

  # The following example enriches each event with host metadata.
  - add_host_metadata:
      netinfo.enabled: false

  - drop_fields:
      fields:
        - system.process.cgroup.blkio.id
        - system.process.cgroup.blkio.path
        - system.process.cgroup.blkio.total.bytes
        - system.process.cgroup.blkio.total.ios
        - system.process.cgroup.cpu.cfs.period.us
        - system.process.cgroup.cpu.cfs.quota.us
        - system.process.cgroup.cpu.cfs.shares
        - system.process.cgroup.cpu.id
        - system.process.cgroup.cpu.path
        - system.process.cgroup.cpu.rt.period.us
        - system.process.cgroup.cpu.rt.runtime.us
        - system.process.cgroup.cpu.stats.periods
        - system.process.cgroup.cpu.stats.throttled.ns
        - system.process.cgroup.cpu.stats.throttled.periods
        - system.process.cgroup.cpuacct.id
        - system.process.cgroup.cpuacct.path
        - system.process.cgroup.cpuacct.stats.system.ns
        - system.process.cgroup.cpuacct.stats.user.ns
        - system.process.cgroup.cpuacct.total.ns
        - system.process.cgroup.memory.id
        - system.process.cgroup.memory.kmem.failures
        - system.process.cgroup.memory.kmem.limit.bytes
        - system.process.cgroup.memory.kmem.usage.bytes
        - system.process.cgroup.memory.kmem.usage.max.bytes
        - system.process.cgroup.memory.kmem_tcp.failures
        - system.process.cgroup.memory.kmem_tcp.limit.bytes
        - system.process.cgroup.memory.kmem_tcp.usage.bytes
        - system.process.cgroup.memory.kmem_tcp.usage.max.bytes
        - system.process.cgroup.memory.mem.failures
        - system.process.cgroup.memory.mem.limit.bytes
        - system.process.cgroup.memory.mem.usage.bytes
        - system.process.cgroup.memory.mem.usage.max.bytes
        - system.process.cgroup.memory.memsw.failures
        - system.process.cgroup.memory.memsw.limit.bytes
        - system.process.cgroup.memory.memsw.usage.bytes
        - system.process.cgroup.memory.memsw.usage.max.bytes
        - system.process.cgroup.memory.path
        - system.process.cgroup.memory.stats.active_anon.bytes
        - system.process.cgroup.memory.stats.active_file.bytes
        - system.process.cgroup.memory.stats.cache.bytes
        - system.process.cgroup.memory.stats.hierarchical_memory_limit.bytes
        - system.process.cgroup.memory.stats.hierarchical_memsw_limit.bytes
        - system.process.cgroup.memory.stats.inactive_anon.bytes
        - system.process.cgroup.memory.stats.inactive_file.bytes
        - system.process.cgroup.memory.stats.major_page_faults
        - system.process.cgroup.memory.stats.mapped_file.bytes
        - system.process.cgroup.memory.stats.page_faults
        - system.process.cgroup.memory.stats.pages_in
        - system.process.cgroup.memory.stats.pages_out
        - system.process.cgroup.memory.stats.rss.bytes
        - system.process.cgroup.memory.stats.rss_huge.bytes
        - system.process.cgroup.memory.stats.swap.bytes
        - system.process.cgroup.memory.stats.unevictable.bytes
        - system.process.cgroup.path

  # The following example renames the field a to b:
  - rename:
      ignore_missing: true
      fail_on_error: false
      fields:
        - from: docker.container.labels.io.rancher.container.name
          to: container.name

# The directory from where to read the dashboards. The default is the `kibana`
# folder in the home path.
setup.dashboards.directory: /usr/share/metricbeat/kibana
